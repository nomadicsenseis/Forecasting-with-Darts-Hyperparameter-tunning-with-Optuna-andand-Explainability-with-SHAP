{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (5.9.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly) (8.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: boto3==1.19.12 in /opt/conda/lib/python3.10/site-packages (1.19.12)\n",
      "Requirement already satisfied: botocore<1.23.0,>=1.22.12 in /opt/conda/lib/python3.10/site-packages (from boto3==1.19.12) (1.22.12)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3==1.19.12) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from boto3==1.19.12) (0.5.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.23.0,>=1.22.12->boto3==1.19.12) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.23.0,>=1.22.12->boto3==1.19.12) (1.26.16)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.23.0,>=1.22.12->boto3==1.19.12) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /opt/conda/lib/python3.10/site-packages (from s3fs) (1.22.12)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from s3fs) (2022.7.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (1.26.16)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install plotly\n",
    "!pip install boto3==1.19.12\n",
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import os\n",
    "import numpy as np\n",
    "import xlsxwriter\n",
    "import datetime\n",
    "import boto3\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nps_path = 's3://iberia-data-lake/customer/nps_surveys/export_historic/insert_date_ci=2023-08-07/'\n",
    "iberia_kpis_path = 's3://iberia-data-lake/customer/one_shot/iberia_kpis_monthly/'\n",
    "dict_path= 's3://iberia-data-lake/customer/nps_surveys/nps_dictionaries'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_touchpoints = pd.read_csv(f'{dict_path}/nps_dictionary_touchpoints.csv')\n",
    "df_issues = pd.read_csv(f'{dict_path}/nps_dictionary_issues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_touchpoints_issues = df_touchpoints.merge(df_issues[[\"associated_touchpoint\", \"issue_type_2\"]], left_on = \"survey_maritz_name\", right_on = \"associated_touchpoint\", how = \"left\")\n",
    "df_touchpoints_issues[\"issue_type_2\"] = df_touchpoints_issues[\"issue_type_2\"].str.lower()\n",
    "df_touchpoints_issues = df_touchpoints_issues.loc[(df_touchpoints_issues[\"issue_type_2\"].notnull())\n",
    "                    & (df_touchpoints_issues[\"issue_type_2\"] != \"issuewifi\")].drop([\"survey_maritz_name\"], axis = 1)\n",
    "issues_list = [i for i in df_touchpoints_issues[\"issue_type_2\"].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "def read_csv_files_from_s3(s3_path):\n",
    "    # Initialize Boto3 client for S3\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Extract bucket name and prefix from the S3 path\n",
    "    bucket, prefix = s3_path.replace('s3://', '').split('/', 1)\n",
    "\n",
    "    # Get a list of all keys (object paths) under the specified S3 path\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_keys = [item.key for item in s3_resource.Bucket(bucket).objects.filter(Prefix=prefix)]\n",
    "\n",
    "    # Generate a list of full S3 paths for all CSV files\n",
    "    preprocess_paths = [f\"s3://{bucket}/{key}\" for key in s3_keys]\n",
    "\n",
    "    # Read all CSV files into a list of DataFrames\n",
    "    dfs = [pd.read_csv(s3_client.get_object(Bucket=bucket, Key=key)['Body']) for key in s3_keys]\n",
    "\n",
    "    # Concatenate DataFrames into a single DataFrame\n",
    "    df_concatenated = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    return df_concatenated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Readd files from paths\n",
    "df_nps = read_csv_files_from_s3(nps_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_kpis = read_csv_files_from_s3(iberia_kpis_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps['date_flight_local'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_kpis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### NPS main information analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out null tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "condition_1 = (df_nps['operating_airline_code'].isin(['IB', 'YW']))\n",
    "condition_2 = ((df_nps['invitegroup_ib'] != 3) | (df_nps['invitegroup_ib'].isnull()))\n",
    "condition_3 = (df_nps['invitegroup'] == 2)\n",
    "\n",
    "df_nps_tkt = df_nps.loc[condition_1 & (condition_2 & condition_3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_nps_tkt_filtered = df_nps_tkt.dropna(subset=['ticket_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_tkt.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cast and format date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datetime_features = ['date_flight_local', 'scheduled_departure_time_local', 'scheduled_arrival_time_local', 'real_departure_time_local',\n",
    "                     'real_arrival_time_local', 'started']\n",
    "columns_to_cross_kpis=['cabin_in_surveyed_flight','haul']\n",
    "columns_ext = ['tier_level', 'language_code', 'seat_no', 'volume_of_bags', 'number_of_child_in_the_booking', 'number_of_infant_in_the_booking',\n",
    "              'number_of_people_in_the_booking', 'country_code', 'customer_journey_origin', 'customer_journey_destination', 'number_of_flights_in_journey',\n",
    "              'order_of_flight_in_journey', 'marketing_airline_code', 'overall_haul', 'weight_category', 'ff_number', 'ticket_num', 'operating_airline_code',\n",
    "               'nps_category', 'nps_100', 'group_age_survey', 'gender', 'promoter_binary', 'detractor_binary'] # invite_group\n",
    "touchpoints = ['bkg_100_booking', 'bkg_200_journey_preparation', 'pfl_100_checkin', 'pfl_200_security', 'pfl_300_lounge',\n",
    "               'pfl_500_boarding', 'ifl_300_cabin', 'ifl_200_flight_crew_annoucements', 'ifl_600_wifi', 'ifl_500_ife',\n",
    "               'ifl_400_food_drink', 'ifl_100_cabin_crew', 'arr_100_arrivals', 'con_100_connections', 'pun_100_punctuality',\n",
    "               'loy_200_loyalty_programme', 'inm_400_issues_response', 'img_310_ease_contact_phone']\n",
    "survey_fields = ['cla_600_wifi_t_f', 'tvl_journey_reason']\n",
    "\n",
    "for feat in datetime_features:\n",
    "    if feat in ['scheduled_departure_time_local', 'scheduled_arrival_time_local', 'real_departure_time_local', 'real_arrival_time_local','date_flight_local']:\n",
    "        df_nps_tkt[feat] = pd.to_datetime(df_nps_tkt[feat], format=\"%Y%m%d %H:%M:%S\", errors = 'coerce')\n",
    "    else:\n",
    "        df_nps_tkt[feat] = pd.to_datetime(df_nps_tkt[feat], errors = 'ignore')\n",
    "df_nps_tkt['time_spent_hrminsec'] = pd.to_timedelta(df_nps_tkt['time_spent_hrminsec']).dt.total_seconds()\n",
    "df_nps_tkt['started_hour'] = df_nps_tkt['started'].dt.hour\n",
    "df_nps_tkt['year_flight'] = df_nps_tkt['date_flight_local'].dt.year\n",
    "df_nps_tkt['month_flight'] = df_nps_tkt['date_flight_local'].dt.month\n",
    "df_nps_tkt['day_flight'] = df_nps_tkt['date_flight_local'].dt.day\n",
    "df_nps_tkt['weekday_flight'] = df_nps_tkt['date_flight_local'].dt.weekday\n",
    "df_nps_tkt['is_weekend_or_friday_flight'] = df_nps_tkt['weekday_flight'].apply(lambda x: 1 if x in [5, 6,7] else 0)\n",
    "df_nps_tkt['delay_departure'] = (df_nps_tkt['real_departure_time_local'] - df_nps_tkt['scheduled_departure_time_local']).dt.total_seconds()/60\n",
    "df_nps_tkt['delay_arrival'] = (df_nps_tkt['real_arrival_time_local'] - df_nps_tkt['scheduled_arrival_time_local']).dt.total_seconds()/60\n",
    "datetime_features = datetime_features + ['time_spent_hrminsec', 'started_hour', 'year_flight', 'month_flight',\n",
    "                                         'day_flight', 'weekday_flight', 'is_weekend_or_friday_flight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_tkt['date_flight_local'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create flag promoter and detractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_tkt[\"promoter_binary\"] = df_nps_tkt[\"nps_100\"].apply(lambda x: 1 if x == \"Promoter\" else 0)\n",
    "df_nps_tkt[\"detractor_binary\"] = df_nps_tkt[\"nps_100\"].apply(lambda x: 1 if x == \"Detractor\" else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wifi_var(df):\n",
    "\n",
    "    df[\"wifi_not_working\"] = df[\"cla_600_wifi_t_f\"].apply(lambda x: 1 if x in [\"Could not get it to work\", \"No - I could not get it to work\"] else 0)\n",
    "    df[\"wifi_used_success\"] = df[\"cla_600_wifi_t_f\"].apply(lambda x: 1 if x in [\"Yes\", \"Yes, but not enough\"] else 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def group_journey_reason(df):\n",
    "    \n",
    "    df[\"tvl_journey_reason\"] = df[\"tvl_journey_reason\"].apply(lambda x: 1 if x in [\"Business\", \"Business/work\"] else 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_tkt = wifi_var(df_nps_tkt)\n",
    "df_nps_tkt = group_journey_reason(df_nps_tkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Select column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_select = datetime_features + columns_ext + touchpoints + ['delay_arrival', 'delay_departure', 'ticket_price']+columns_to_cross_kpis+['monthly_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_tkt = df_nps_tkt[columns_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_tkt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_tkt['haul'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_date_flight_local = df_nps_tkt['date_flight_local'].min()\n",
    "print(f\"The first date in 'date_flight_local' variable is: {first_date_flight_local}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check to maximum time spend on survey per nps score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_tkt.groupby('nps_100')['time_spent_hrminsec'].max().plot()\n",
    "plt.title('Time spent in survey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets check how the touchpoints scores correlated with the NPS on the client level. Later we will check the same thing with the aggregated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correlation_fields = ['bkg_100_booking', 'bkg_200_journey_preparation', 'pfl_100_checkin', 'pfl_200_security', 'pfl_300_lounge',\n",
    "      'pfl_500_boarding', 'ifl_300_cabin', 'ifl_200_flight_crew_annoucements', 'ifl_600_wifi', 'ifl_500_ife',\n",
    "      'ifl_400_food_drink', 'ifl_100_cabin_crew', 'arr_100_arrivals', 'con_100_connections', 'pun_100_punctuality',\n",
    "      'loy_200_loyalty_programme', 'inm_400_issues_response', 'img_310_ease_contact_phone', 'nps_100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_corr=df_nps_tkt[correlation_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nps_corr.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Agregate on week, calculate NPS, NPS_adjusted, stats of the touchpoints and stats of the touchpoints adjusted, and check time dependencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_nps(promoters, detractors, passives):\n",
    "    total_responses = promoters + detractors + passives\n",
    "    nps = 100 * (promoters - detractors) / total_responses\n",
    "    return nps\n",
    "\n",
    "def calculate_aggregated_features(df, variables, time_frequency='M', calculate_satisfaction=True):\n",
    "    touchpoints = [var for var in variables if calculate_satisfaction]\n",
    "    non_touchpoints = [var for var in variables if not calculate_satisfaction]\n",
    "\n",
    "    # Step 1: Resample data to the specified time frequency and calculate NPS for each time period\n",
    "    resampled_data = df.resample(time_frequency, on='date_flight_local').agg({\n",
    "        'nps_100': lambda x: calculate_nps(sum((x == 9) | (x == 10)), sum(x <= 6), sum((x == 7) | (x == 8)))\n",
    "    })\n",
    "\n",
    "    # Step 2: Calculate the aggregated features for each categorical variable\n",
    "    aggregated_features_data = pd.DataFrame({\n",
    "        'feature_date': resampled_data.index,\n",
    "        'NPS': resampled_data['nps_100']\n",
    "    })\n",
    "\n",
    "    for col in touchpoints:\n",
    "        aggregated_features_data[f'{col}_satisfaction'] = df.resample(time_frequency, on='date_flight_local')[col].apply(lambda x: x[x >= 8].count() / x.count())\n",
    "        aggregated_features_data[f'{col}_sum'] = df.resample(time_frequency, on='date_flight_local')[col].sum()\n",
    "        aggregated_features_data[f'{col}_std'] = df.resample(time_frequency, on='date_flight_local')[col].std()\n",
    "        aggregated_features_data[f'{col}_mean'] = df.resample(time_frequency, on='date_flight_local')[col].mean()\n",
    "        aggregated_features_data[f'{col}_not_nulls'] = df.resample(time_frequency, on='date_flight_local')[col].apply(lambda x: x.notnull().sum())\n",
    "\n",
    "    for col in non_touchpoints:\n",
    "        aggregated_features_data[f'{col}_sum'] = df.resample(time_frequency, on='date_flight_local')[col].sum()\n",
    "        aggregated_features_data[f'{col}_std'] = df.resample(time_frequency, on='date_flight_local')[col].std()\n",
    "        aggregated_features_data[f'{col}_mean'] = df.resample(time_frequency, on='date_flight_local')[col].mean()\n",
    "        aggregated_features_data[f'{col}_not_nulls'] = df.resample(time_frequency, on='date_flight_local')[col].apply(lambda x: x.notnull().sum())\n",
    "        aggregated_features_data[f'{col}_max'] = df.resample(time_frequency, on='date_flight_local')[col].max()  # Add max calculation\n",
    "        aggregated_features_data[f'{col}_min'] = df.resample(time_frequency, on='date_flight_local')[col].min()  # Add min calculation\n",
    "\n",
    "    return aggregated_features_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_correlated_features(df, string_ending, n):\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Get the correlation values of each feature with NPS and sort them in descending order\n",
    "    nps_correlations = correlation_matrix['NPS'].drop('NPS').sort_values(ascending=False)\n",
    "\n",
    "    # Filter the correlations for features that end with the specified \"_string\"\n",
    "    filtered_correlations = nps_correlations[nps_correlations.index.str.endswith(string_ending)]\n",
    "\n",
    "    # Get the top 'n' features with the highest correlations among \"_string\" features\n",
    "    top_n_correlated_features = filtered_correlations.nlargest(n).index.tolist()\n",
    "\n",
    "    return top_n_correlated_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_time_series_by_years(dataframe, numerical_vars_list, years):\n",
    "    df=dataframe.copy()\n",
    "    \n",
    "    num_rows = len(numerical_vars_list)\n",
    "    num_years = len(years)\n",
    "\n",
    "    # Normalize the selected numeric variables and 'NPS' column\n",
    "    for var in df.columns:\n",
    "        if var.startswith('NPS') or var in numerical_vars_list[0]:\n",
    "            df[var] = (df[var] - df[var].min()) / (df[var].max() - df[var].min())\n",
    "\n",
    "    # Melt the DataFrame to combine numerical variable columns into a single column\n",
    "    melted_data = pd.melt(df, id_vars=['feature_date'], value_vars=numerical_vars_list[0] + ['NPS'],\n",
    "                          var_name='variable', value_name='value')\n",
    "\n",
    "    # Filter data for the specified years\n",
    "    melted_data_years = melted_data[melted_data['feature_date'].dt.year.isin(years)]\n",
    "    \n",
    "    # Create subplots for each row and each year\n",
    "    fig, axes = plt.subplots(num_rows, num_years, figsize=(20 * num_years, 12 * num_rows), sharex='col', sharey='row')\n",
    "\n",
    "    for row_idx, numerical_vars in enumerate(numerical_vars_list):\n",
    "        for col_idx, year in enumerate(years):\n",
    "            # Plot for the current row and year\n",
    "            sns.lineplot(x='feature_date', y='value', hue='variable', data=melted_data_years[melted_data_years['feature_date'].dt.year == year], ax=axes[row_idx, col_idx])\n",
    "            axes[row_idx, col_idx].set_title(f'Normalized Time Series for NPS and Numerical Variables (Year {year})')\n",
    "            axes[row_idx, col_idx].set_xlabel('Date')\n",
    "            axes[row_idx, col_idx].set_ylabel('Normalized Values')\n",
    "            axes[row_idx, col_idx].tick_params(axis='x', rotation=45)  # Rotate x-axis labels for better readability\n",
    "            axes[row_idx, col_idx].xaxis.set_major_locator(plt.MaxNLocator(nbins=10))  # Set the number of x-axis tick marks\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from calendar import month_name as mn\n",
    "\n",
    "def plot_nps_time_series(df, years):\n",
    "    # Filter data for the specified years\n",
    "    melted_data_years = df[df['feature_date'].dt.year.isin(years)]\n",
    "\n",
    "    # Create a new 'month' column to represent the month of each feature_date\n",
    "    melted_data_years['month'] = melted_data_years['feature_date'].dt.month\n",
    "\n",
    "    # Convert the 'month' column to categorical and ordered\n",
    "    months = mn[1:]\n",
    "    melted_data_years['month'] = pd.Categorical(melted_data_years['month'], categories=range(1, 13), ordered=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for year in years:\n",
    "        # Filter data for the current year and 'NPS' column\n",
    "        data_to_plot = melted_data_years[melted_data_years['feature_date'].dt.year == year]\n",
    "        # Convert the 'month' and 'NPS' columns to regular arrays\n",
    "        months_arr = data_to_plot['month'].values\n",
    "        nps_arr = data_to_plot['NPS'].values\n",
    "        # Plot the line for the current year and 'NPS'\n",
    "        ax.plot(months_arr, nps_arr, marker='o', label=f'Year {year}')\n",
    "\n",
    "    ax.set_title('NPS Time Series')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('NPS Values')\n",
    "    ax.set_xticks(range(1, 13))\n",
    "    ax.set_xticklabels(months, rotation=45)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_time_series_by_vars(dataframe, numerical_vars_list, years):\n",
    "    df=dataframe.copy()\n",
    "    # Normalize the selected numeric variables and 'NPS' column\n",
    "    for var in df.columns:\n",
    "        if var.startswith('NPS') or var in numerical_vars_list[0]:\n",
    "            df[var] = (df[var] - df[var].min()) / (df[var].max() - df[var].min())\n",
    "\n",
    "    # Melt the DataFrame to combine numerical variable columns into a single column\n",
    "    melted_data = pd.melt(df, id_vars=['feature_date'], value_vars=numerical_vars_list[0] + ['NPS'],\n",
    "                          var_name='variable', value_name='value')\n",
    "\n",
    "    # Filter data for the specified years\n",
    "    melted_data_years = melted_data[melted_data['feature_date'].dt.year.isin(years)]\n",
    "\n",
    "    # Create a new 'month' column to represent the month of each feature_date\n",
    "    melted_data_years['month'] = melted_data_years['feature_date'].dt.month\n",
    "\n",
    "    # Convert the 'month' column to categorical and ordered\n",
    "    months = mn[1:]\n",
    "    melted_data_years['month'] = pd.Categorical(melted_data_years['month'], categories=range(1, 13), ordered=True)\n",
    "\n",
    "    num_groups = len(numerical_vars_list)\n",
    "    \n",
    "    for group_idx, numerical_vars in enumerate(numerical_vars_list):\n",
    "        num_vars = len(numerical_vars)\n",
    "        fig, axes = plt.subplots(1, num_vars, figsize=(5 * num_vars, 5), sharex='col', sharey='row')\n",
    "\n",
    "        for var_idx, var in enumerate(numerical_vars):\n",
    "            for year in years:\n",
    "                # Filter data for the current year and variable\n",
    "                data_to_plot = melted_data_years[\n",
    "                    (melted_data_years['feature_date'].dt.year == year) &\n",
    "                    (melted_data_years['variable'] == var)\n",
    "                ]\n",
    "                # Convert the 'month' and 'value' columns to regular arrays\n",
    "                months_arr = data_to_plot['month'].values\n",
    "                values_arr = data_to_plot['value'].values\n",
    "                # Plot the line for the current year and variable\n",
    "                axes[var_idx].plot(months_arr, values_arr, marker='o', label=f'Year {year}')\n",
    "                axes[var_idx].set_title(f'{var}')\n",
    "                axes[var_idx].set_xlabel('Month')\n",
    "                axes[var_idx].set_ylabel('Normalized Values')\n",
    "                axes[var_idx].set_xticks(range(1, 13))\n",
    "                axes[var_idx].set_xticklabels(months, rotation=45)\n",
    "                axes[var_idx].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def plot_nps_time_series_with_diff(df, years):\n",
    "\n",
    "    # Melt the DataFrame to combine numerical variable columns into a single column\n",
    "    melted_data = pd.melt(df, id_vars=['feature_date'], value_vars=['NPS'],\n",
    "                          var_name='variable', value_name='value')\n",
    "\n",
    "    # Filter data for the specified years\n",
    "    melted_data_years = melted_data[melted_data['feature_date'].dt.year.isin(years)]\n",
    "\n",
    "    # Create a new 'month' column to represent the month of each feature_date\n",
    "    melted_data_years['month'] = melted_data_years['feature_date'].dt.month\n",
    "\n",
    "    # Convert the 'month' column to categorical and ordered\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    melted_data_years['month'] = pd.Categorical(melted_data_years['month'], categories=range(1, 13), ordered=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    for year in years:\n",
    "        # Filter data for the current year and 'NPS' column\n",
    "        data_to_plot = melted_data_years[melted_data_years['feature_date'].dt.year == year]\n",
    "        # Convert the 'month' and 'NPS' columns to regular arrays\n",
    "        months_arr = data_to_plot['month'].values\n",
    "        nps_arr = data_to_plot['value'].values\n",
    "        # Plot the line for the current year and 'NPS' values\n",
    "        ax.plot(months_arr, nps_arr, marker='o', label=f'NPS (Year {year})')\n",
    "\n",
    "        # Calculate the differences between consecutive NPS values\n",
    "        nps_diff = nps_arr[1:] - nps_arr[:-1]\n",
    "        # Plot the line for the differences\n",
    "        ax.plot(months_arr[1:], nps_diff, marker='o', linestyle='dashed', label=f'Differences (Year {year})')\n",
    "\n",
    "    ax.set_title('NPS Time Series with Differences')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('NPS Values / Differences')\n",
    "    ax.legend()\n",
    "\n",
    "    # Compute correlations between NPS values of the two years\n",
    "    nps_values_1 = melted_data_years[melted_data_years['feature_date'].dt.year == years[0]]['value'].values\n",
    "    nps_values_2 = melted_data_years[melted_data_years['feature_date'].dt.year == years[1]]['value'].values\n",
    "    pearson_corr_values, _ = pearsonr(nps_values_1, nps_values_2)\n",
    "    spearman_corr_values, _ = spearmanr(nps_values_1, nps_values_2)\n",
    "\n",
    "    # Compute correlations between NPS differences of the two years\n",
    "    nps_diff_1 = nps_values_1[1:] - nps_values_1[:-1]\n",
    "    nps_diff_2 = nps_values_2[1:] - nps_values_2[:-1]\n",
    "    pearson_corr_diff, _ = pearsonr(nps_diff_1, nps_diff_2)\n",
    "    spearman_corr_diff, _ = spearmanr(nps_diff_1, nps_diff_2)\n",
    "\n",
    "    print(f'Correlations between NPS values for Year {years[0]} and Year {years[1]}:')\n",
    "    print(f'Pearson correlation: {pearson_corr_values:.2f}')\n",
    "    print(f'Spearman correlation: {spearman_corr_values:.2f}')\n",
    "\n",
    "    print(f'Correlations between NPS differences for Year {years[0]} and Year {years[1]}:')\n",
    "    print(f'Pearson correlation: {pearson_corr_diff:.2f}')\n",
    "    print(f'Spearman correlation: {spearman_corr_diff:.2f}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PLOT DIFFERENT VARIABLES AND DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregate touchpoints on a monthly level\n",
    "aggregated_features_data_monthly = calculate_aggregated_features(df_nps_tkt, ['delay_arrival', 'delay_departure', 'ticket_price'], time_frequency='M', calculate_satisfaction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aggregated_features_data_w = calculate_aggregated_features(df_nps_tkt, touchpoints, time_frequency='W', calculate_satisfaction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agregate touchpoints on a monthly level\n",
    "aggregated_touchpoints_data_monthly = calculate_aggregated_features(df_nps_tkt, touchpoints, time_frequency='M', calculate_satisfaction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aggregated_touchpoints_data_monthly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'aggregated_features_data_monthly' contains the DataFrame\n",
    "years_to_filter = [2019,2022,2023,2020,2021]\n",
    "filtered_data = aggregated_features_data_w[aggregated_features_data_w['feature_date'].dt.year.isin(years_to_filter)]\n",
    "data_length = len(filtered_data)\n",
    "print(data_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Top correlated (with NPS) features\n",
    "top_3_correlated_features = get_top_correlated_features(aggregated_features_data_monthly, \"\", 3)\n",
    "print(top_3_correlated_features)\n",
    "\n",
    "#Top corelated features among the _satisfaction ones\n",
    "top_3_mean_features = get_top_correlated_features(aggregated_features_data_monthly, \"_mean\", 3)\n",
    "print(top_3_mean_features)\n",
    "\n",
    "# Top correlated features among the _not_nulls ones\n",
    "top_3_max_features = get_top_correlated_features(aggregated_features_data_monthly, \"_max\", 3)\n",
    "print(top_3_max_features)\n",
    "\n",
    "#Plot timeseries for NPS and the most correlated features in years 2019 and 2022\n",
    "years_to_plot = [2022, 2021, 2020, 2019]  # Add any arbitrary number of years to plot\n",
    "numerical_vars_list_to_plot = [top_3_correlated_features, top_3_mean_features, top_3_max_features]  # Add any list of numeric variables to plot\n",
    "plot_time_series_by_years(aggregated_features_data_monthly, numerical_vars_list_to_plot, years_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Top correlated (with NPS) features\n",
    "top_3_correlated_touchpoints = get_top_correlated_features(aggregated_touchpoints_data_monthly, \"\", 3)\n",
    "print(top_3_correlated_touchpoints)\n",
    "\n",
    "#Top corelated features among the _satisfaction ones\n",
    "top_3_satisfaction_touchpoints = get_top_correlated_features(aggregated_touchpoints_data_monthly, \"_satisfaction\", 3)\n",
    "print(top_3_satisfaction_touchpoints)\n",
    "\n",
    "# Top correlated features among the _not_nulls ones\n",
    "top_3_not_nulls_touchpoints = get_top_correlated_features(aggregated_touchpoints_data_monthly, \"_not_nulls\", 3)\n",
    "print(top_3_not_nulls_touchpoints)\n",
    "\n",
    "#Plot timeseries for NPS and the most correlated features in years 2019 and 2022\n",
    "years_to_plot = [2022, 2021, 2020, 2019]  # Add any arbitrary number of years to plot\n",
    "touchpoints_list_to_plot = [top_3_correlated_touchpoints, top_3_not_nulls_touchpoints, top_3_satisfaction_touchpoints]  # Add any list of numeric variables to plot\n",
    "plot_time_series_by_years(aggregated_touchpoints_data_monthly, touchpoints_list_to_plot, years_to_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming you have the DataFrame aggregated_features_data_monthly and a list of numeric variables top_3_not_nulls_features\n",
    "years_to_plot = [2022, 2019]  # Add any arbitrary number of years to plot\n",
    "numeric_vars_to_plot = [top_3_correlated_features,top_3_mean_features,top_3_max_features]  \n",
    "\n",
    "for numeric_vars_list in numeric_vars_to_plot:\n",
    "    plot_time_series_by_vars(aggregated_features_data_monthly, [numeric_vars_list], years_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming you have the DataFrame aggregated_features_data_monthly and a list of numeric variables top_3_not_nulls_features\n",
    "years_to_plot = [2022, 2021, 2020, 2019]  # Add any arbitrary number of years to plot\n",
    "touchpoints_to_plot = [top_3_correlated_touchpoints,top_3_satisfaction_touchpoints,top_3_not_nulls_touchpoints]  # Add a list of numeric variables to plot (e.g., [['numeric_var_1', 'numeric_var_2']])\n",
    "\n",
    "for touchpoints_list in touchpoints_to_plot:\n",
    "    plot_time_series_by_vars(aggregated_touchpoints_data_monthly, [touchpoints_list], years_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_nps_time_series(aggregated_touchpoints_data_monthly, [2019,2022,2023])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nps_bi = pd.read_excel('data.xlsx')\n",
    "nps_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from calendar import month_name as mn\n",
    "\n",
    "def plot_nps_time_series(df, nps_bi, years):\n",
    "    # Filter data for the specified years\n",
    "    melted_data_years = df[df['feature_date'].dt.year.isin(years)]\n",
    "\n",
    "    # Create a new 'month' column to represent the month of each feature_date\n",
    "    melted_data_years['month'] = melted_data_years['feature_date'].dt.month\n",
    "\n",
    "    # Convert the 'month' column to categorical and ordered\n",
    "    months = mn[1:]\n",
    "    melted_data_years['month'] = pd.Categorical(melted_data_years['month'], categories=range(1, 13), ordered=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for year in years:\n",
    "        # Filter data for the current year and 'NPS' column\n",
    "        data_to_plot = melted_data_years[melted_data_years['feature_date'].dt.year == year]\n",
    "        # Convert the 'month' and 'NPS' columns to regular arrays\n",
    "        months_arr = data_to_plot['month'].values\n",
    "        nps_arr = data_to_plot['NPS'].values\n",
    "        # Plot the line for the current year and 'NPS'\n",
    "        ax.plot(months_arr, nps_arr, marker='o', label=f'Year {year}')\n",
    "\n",
    "        # Plot 'nps_bi' data for the current year\n",
    "        nps_bi_year = f'NPS {year}'\n",
    "        if nps_bi_year in nps_bi.columns:\n",
    "            ax.plot(months_arr, nps_bi[nps_bi_year], marker='x', label=f'Year {year} (Marketing)', linestyle='dashed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from calendar import month_name as mn\n",
    "\n",
    "def plot_nps_time_series(df, nps_bi, years):\n",
    "    # Filter data for the specified years\n",
    "    melted_data_years = df[df['feature_date'].dt.year.isin(years)]\n",
    "\n",
    "    # Create a new 'month' column to represent the month of each feature_date\n",
    "    melted_data_years['month'] = melted_data_years['feature_date'].dt.month\n",
    "\n",
    "    # Convert the 'month' column to categorical and ordered\n",
    "    months = mn[1:]\n",
    "    melted_data_years['month'] = pd.Categorical(melted_data_years['month'], categories=range(1, 13), ordered=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for year in years:\n",
    "        # Filter data for the current year and 'NPS' column\n",
    "        data_to_plot = melted_data_years[melted_data_years['feature_date'].dt.year == year]\n",
    "        # Convert the 'month' and 'NPS' columns to regular arrays\n",
    "        months_arr = data_to_plot['month'].values\n",
    "        nps_arr = data_to_plot['NPS'].values\n",
    "        # Plot the line for the current year and 'NPS'\n",
    "        ax.plot(months_arr, nps_arr, marker='o', label=f'Year {year}')\n",
    "\n",
    "        # Plot 'nps_bi' data for the current year\n",
    "        nps_bi_year = f'NPS {year}'\n",
    "        if nps_bi_year in nps_bi.columns:\n",
    "            ax.plot(months_arr, nps_bi[nps_bi_year], marker='x', label=f'Year {year} (Marketing)', linestyle='dashed')\n",
    "\n",
    "    ax.set_title('NPS Time Series')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('NPS Values')\n",
    "    ax.set_xticks(range(1, 13))\n",
    "    ax.set_xticklabels(months, rotation=45)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'nps_bi' is your marketing team's DataFrame\n",
    "# and 'df' is your original DataFrame\n",
    "#plot_nps_time_series(aggregated_touchpoints_data_monthly, nps_bi, years=[2019, 2022, 2023])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the NPS comparison and differences\n",
    "plot_nps_time_series_with_diff(aggregated_touchpoints_data_monthly, [2019, 2020,2021, 2022])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_nps_time_series_with_diff(aggregated_touchpoints_data_monthly, [2019, 2018,2017])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Combine monthly aggregates of NPS with Iberia KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aggregated_features_data_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_kpis['haul'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to calculate NPS\n",
    "def calculate_nps(promoters, detractors, passives):\n",
    "    total_responses = promoters + detractors + passives\n",
    "    nps = 100 * (promoters - detractors) / total_responses\n",
    "    return nps\n",
    "\n",
    "def calculate_aggregated_features_with_cross_kpis(df, variables, columns_to_cross_kpis, time_frequency='M', calculate_satisfaction=True):\n",
    "    touchpoints = [var for var in variables if calculate_satisfaction]\n",
    "    non_touchpoints = [var for var in variables if not calculate_satisfaction]\n",
    "\n",
    "    # Combine the values of columns_to_cross_kpis into a new column 'cross_kpis'\n",
    "    df['cross_kpis'] = df[columns_to_cross_kpis].apply(lambda x: '_'.join(x.values.astype(str)), axis=1)\n",
    "\n",
    "    # Group the data by 'cross_kpis' and 'date_flight_local' using pd.Grouper on the entire DataFrame\n",
    "    grouped_data = df.groupby([pd.Grouper(key='cross_kpis'), pd.Grouper(key='date_flight_local', freq=time_frequency)], group_keys=False)\n",
    "\n",
    "    # Calculate the aggregated features for each group\n",
    "    aggregated_features_data = pd.DataFrame()\n",
    "    aggregated_features_data['NPS'] = grouped_data.apply(lambda x: calculate_nps(\n",
    "        sum((x['nps_100'] == 9) | (x['nps_100'] == 10)),\n",
    "        sum(x['nps_100'] <= 6),\n",
    "        sum((x['nps_100'] == 7) | (x['nps_100'] == 8))\n",
    "    ))\n",
    "\n",
    "    for col in touchpoints + non_touchpoints:\n",
    "        aggregated_features_data[f'{col}_sum'] = grouped_data[col].sum()\n",
    "        aggregated_features_data[f'{col}_std'] = grouped_data[col].std()\n",
    "        aggregated_features_data[f'{col}_mean'] = grouped_data[col].mean()\n",
    "        aggregated_features_data[f'{col}_not_nulls'] = grouped_data[col].apply(lambda x: x.notnull().sum())\n",
    "        if col in touchpoints:\n",
    "            aggregated_features_data[f'{col}_satisfaction'] = grouped_data[col].apply(lambda x: x[x >= 8].count() / x.count())\n",
    "        else:\n",
    "            aggregated_features_data[f'{col}_max'] = grouped_data[col].max()\n",
    "            aggregated_features_data[f'{col}_min'] = grouped_data[col].min()\n",
    "            \n",
    "\n",
    "    # Reset the index of aggregated_features_data\n",
    "    aggregated_features_data.reset_index(inplace=True)\n",
    "\n",
    "    # Split the 'cross_kpis' column back into individual columns and concatenate them to aggregated_features_data\n",
    "    split_columns = aggregated_features_data['cross_kpis'].str.split('_', expand=True)\n",
    "    split_columns.columns = columns_to_cross_kpis\n",
    "    aggregated_features_data = pd.concat([split_columns, aggregated_features_data], axis=1)\n",
    "    \n",
    "    # Drop the temporary 'cross_kpis' column\n",
    "    aggregated_features_data.drop(columns=['cross_kpis'], inplace=True)\n",
    "    \n",
    "    return aggregated_features_data\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have already defined the required variables 'touchpoints', 'non_touchpoints', 'df_nps_tkt', and 'columns_to_cross_kpis'\n",
    "result_df = calculate_aggregated_features_with_cross_kpis(df_nps_tkt, touchpoints, columns_to_cross_kpis, time_frequency='M', calculate_satisfaction=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_kpis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_kpis['cabin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_kpis['haul'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df_kpis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df['cabin_in_surveyed_flight'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df['haul'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(result_df[result_df['date_flight_local'].dt.year >= 2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df['cabin_in_surveyed_flight']=result_df['cabin_in_surveyed_flight'].replace('Business','Premium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df[result_df['date_flight_local'].dt.year >= 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the 'date_flight_local' column in result_df to separate 'year' and 'month' columns\n",
    "result_df['year'] = result_df['date_flight_local'].dt.year\n",
    "result_df['month'] = result_df['date_flight_local'].dt.month\n",
    "\n",
    "# Perform the merge based on the specified columns\n",
    "merged_df = result_df.merge(df_kpis, \n",
    "                            left_on=['year', 'month', 'cabin_in_surveyed_flight', 'haul'], \n",
    "                            right_on=['year', 'month', 'cabin', 'haul'], \n",
    "                            how='inner')\n",
    "\n",
    "# Drop the redundant columns from the merged dataframe\n",
    "merged_df.drop(columns=['cabin_in_surveyed_flight'], inplace=True)\n",
    "\n",
    "# Reorder the columns to place 'year', 'month', 'cabin', and 'haul' at the beginning\n",
    "cols_to_move = ['date_flight_local','year', 'month', 'cabin', 'haul']\n",
    "merged_df = merged_df[cols_to_move + [col for col in merged_df.columns if col not in cols_to_move]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df.to_csv('merged_df_complete', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Top correlated (with NPS) features\n",
    "top_5_correlated_touchpoints = get_top_correlated_features(aggregated_touchpoints_data_monthly, \"\", 5)\n",
    "print(top_5_correlated_touchpoints)\n",
    "\n",
    "#Top corelated features among the _satisfaction ones\n",
    "top_5_satisfaction_touchpoints = get_top_correlated_features(aggregated_touchpoints_data_monthly, \"_satisfaction\", 5)\n",
    "print(top_5_satisfaction_touchpoints)\n",
    "\n",
    "# Top correlated features among the _not_nulls ones\n",
    "top_5_not_nulls_touchpoints = get_top_correlated_features(aggregated_touchpoints_data_monthly, \"_not_nulls\", 5)\n",
    "print(top_5_not_nulls_touchpoints)\n",
    "\n",
    "#Plot timeseries for NPS and the most correlated features in years 2019 and 2022\n",
    "years_to_plot = [2022, 2021, 2020, 2019]  # Add any arbitrary number of years to plot\n",
    "touchpoints_list_to_plot = [top_5_correlated_touchpoints, top_5_not_nulls_touchpoints, top_5_satisfaction_touchpoints]  # Add any list of numeric variables to plot\n",
    "plot_time_series_by_years(aggregated_touchpoints_data_monthly, touchpoints_list_to_plot, years_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the columns to keep in the desired order\n",
    "cols_to_keep = ['date_flight_local','year', 'month', 'cabin', 'haul'] + top_5_satisfaction_touchpoints + ['load_factor', 'mean_price', 'deviation_price'] + [col for col in merged_df.columns if col.startswith('otp')]+['NPS']\n",
    "\n",
    "# Keep only the specified columns in the desired order\n",
    "merged_df = merged_df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df.to_csv('merged_df', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine 'year' and 'month' columns into a new column 'year_month' in df_kpis\n",
    "df_kpis['year_month'] = df_kpis['year'].astype(str) + '-' + df_kpis['month'].astype(str)\n",
    "\n",
    "# Combine 'year' and 'month' columns into a new column 'year_month' in merged_df\n",
    "merged_df['year_month'] = merged_df['year'].astype(str) + '-' + merged_df['month'].astype(str)\n",
    "\n",
    "# Check which combinations are missing in merged_df\n",
    "missing_combinations = df_kpis[~df_kpis['year_month'].isin(merged_df['year_month'])]\n",
    "\n",
    "# Display the missing combinations\n",
    "print(missing_combinations)\n",
    "\n",
    "merged_df.drop(columns=['year_month'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MVP model trainning using Darts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "merged_df=pd.read_csv('merged_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install darts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import darts\n",
    "from darts import TimeSeries\n",
    "from darts.utils.timeseries_generation import (\n",
    "    gaussian_timeseries,\n",
    "    linear_timeseries,\n",
    "    sine_timeseries,\n",
    ")\n",
    "\n",
    "from darts.metrics import mape, smape, mae\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from darts.models import LightGBMModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### First approach: 1 dataframe, 5 group cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the categorical columns\n",
    "categorical_cols = [\"cabin\",\"haul\"]\n",
    "\n",
    "merged_df['date_flight_local'] = pd.to_datetime(merged_df['date_flight_local'])\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "merged_df_encoded = pd.get_dummies(merged_df, columns=categorical_cols)\n",
    "\n",
    "merged_df_encoded = merged_df_encoded[merged_df_encoded['date_flight_local'].dt.year.isin([2019, 2022, 2023])]\n",
    "import pandas as pd\n",
    "\n",
    "merged_df_encoded['date_flight_local'] = merged_df_encoded['date_flight_local'].apply(lambda x: x.replace(year=2021) if x.year == 2019 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "top_5_satisfaction_touchpoints=['pun_100_punctuality_satisfaction', 'ifl_100_cabin_crew_satisfaction', 'pfl_500_boarding_satisfaction', 'ifl_300_cabin_satisfaction', 'bkg_200_journey_preparation_satisfaction']\n",
    "features=top_5_satisfaction_touchpoints + ['load_factor', 'mean_price', 'deviation_price'] + [col for col in merged_df.columns if col.startswith('otp')]\n",
    "\n",
    "NPS_ts= TimeSeries.from_group_dataframe(\n",
    "    merged_df_encoded,\n",
    "    time_col=\"date_flight_local\",\n",
    "    group_cols=[\"cabin_Economy\",\"cabin_Premium\",\"cabin_Premium Economy\",\"haul_SH\",\"haul_LH\"],  # individual time series are extracted by grouping `df` by `group_cols`\n",
    "    value_cols=['NPS'],\n",
    "    fillna_value=0,\n",
    "    freq='M'# optionally, specify the time varying columns\n",
    ")\n",
    "\n",
    "covariates_ts = TimeSeries.from_group_dataframe(\n",
    "    merged_df_encoded,\n",
    "    time_col=\"date_flight_local\",\n",
    "    group_cols=[\"cabin_Economy\",\"cabin_Premium\",\"cabin_Premium Economy\",\"haul_SH\",\"haul_LH\"],\n",
    "    value_cols=features,  # Use only the features as covariates\n",
    "    fillna_value=0,\n",
    "    freq='M'\n",
    ")\n",
    "\n",
    "train_nps = []\n",
    "val_nps = []\n",
    "train_covariates = []\n",
    "val_covariates = []\n",
    "\n",
    "for ts in NPS_ts:\n",
    "    train_nps.append(ts[:-12])  # Append the first part of the time series to train_nps\n",
    "    val_nps.append(ts[-12:])   # Append the last 12 elements (validation data) to val_nps\n",
    "\n",
    "for covariate_ts in covariates_ts:\n",
    "    train_covariates.append(covariate_ts[:-12])  # Append the first part of the covariate time series to train_covariates\n",
    "    val_covariates.append(covariate_ts[-12:])    # Append the last 12 elements (validation data) to val_covariates\n",
    "    \n",
    "from darts.models import LightGBMModel\n",
    "\n",
    "quantiles = [0.25, 0.5, 0.75]\n",
    "\n",
    "model = LightGBMModel(\n",
    "    lags=12,\n",
    "    quantiles=quantiles,\n",
    "    lags_future_covariates=[-12],\n",
    "    likelihood=\"quantile\",\n",
    "    fit_intercept=False,\n",
    "    output_chunk_lenth=12,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_nps, future_covariates=train_covariates\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"\\n{len(NPS_ts)} series were extracted from the input DataFrame\")\n",
    "for i, ts in enumerate(NPS_ts):\n",
    "    print(f\"Series {i}\")\n",
    "    ts[\"NPS\"].plot(label=f\"NPS_series_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"\\n{len(covariates_ts)} series were extracted from the input DataFrame\")\n",
    "for i, ts in enumerate(covariates_ts):\n",
    "    print(f\"Series {i}\")\n",
    "    ts[\"load_factor\"].plot(label=f\"NPS_series_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pred_nps = model.predict(\n",
    "    series=train_nps,\n",
    "    future_covariates=train_covariates,\n",
    "    n=12,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 15))\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(5, 1, i + 1)\n",
    "    NPS_ts[i].plot(label=\"Actual NPS\")\n",
    "    pred_nps[i].plot(label=\"Forecast NPS\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Plot for Index {i}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"forecast_plots.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mape(NPS_ts[0], pred_nps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(train_nps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "start_index = (len(train_nps[0]) - 11)/len(train_nps[0])\n",
    "backtest = model.historical_forecasts(\n",
    "    train_nps, future_covariates=train_covariates, start=start_index, forecast_horizon=12, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    mape_value = mape(backtest[i], train_nps[i])\n",
    "    print(\"MAPE = %.2f\" % mape_value)\n",
    "    \n",
    "    plt.figure(figsize=(8,4))\n",
    "    train_nps[i].plot(label=\"Train NPS\")\n",
    "    backtest[i].plot(label=\"Backtest NPS\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Plot for Index {i}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Second approach: 5 different dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df['date_flight_local']=pd.to_datetime(merged_df['date_flight_local'])\n",
    "\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_dfs = {}\n",
    "\n",
    "# Group the original DataFrame by the 'cabin' and 'haul' columns\n",
    "grouped = merged_df.groupby(['cabin', 'haul'])\n",
    "\n",
    "# Iterate through each group and create a DataFrame\n",
    "for group_name, group_data in grouped:\n",
    "    cabin_value, haul_value = group_name\n",
    "    group_df = group_data.copy()  # Create a copy of the group's data\n",
    "    group_df_name = f'{cabin_value}_{haul_value}_df'  # Generate a unique name\n",
    "    grouped_dfs[group_df_name] = group_df\n",
    "\n",
    "# Now you have a dictionary containing separate DataFrames for each combination of values\n",
    "# Access them using the keys in the dictionary\n",
    "grouped_dfs['Premium Economy_LH_df'] = grouped_dfs['Premium Economy_LH_df'][grouped_dfs['Premium Economy_LH_df']['date_flight_local'].dt.year.isin([2019, 2022, 2023])]\n",
    "\n",
    "grouped_dfs['Premium Economy_LH_df']['date_flight_local'] = grouped_dfs['Premium Economy_LH_df']['date_flight_local'].apply(lambda x: x.replace(year=2021) if x.year == 2019 else x)\n",
    "\n",
    "grouped_dfs['Premium Economy_LH_df'] = grouped_dfs['Premium Economy_LH_df'].reset_index()\n",
    "\n",
    "grouped_dfs['Premium Economy_LH_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "from darts.models import LightGBMModel\n",
    "\n",
    "# Load the data\n",
    "df = grouped_dfs['Premium Economy_LH_df']\n",
    "\n",
    "# Create a new column called \"date\" that contains the datetime objects\n",
    "df['date'] = pd.to_datetime(df['date_flight_local'])\n",
    "\n",
    "df=df.set_index('date')\n",
    "\n",
    "df=df.fillna(0)\n",
    "\n",
    "\n",
    "# Create the target time series\n",
    "nps_ts = TimeSeries.from_series(df['NPS'])\n",
    "\n",
    "# Create the future covariates time series\n",
    "top_5_satisfaction_touchpoints=['pun_100_punctuality_satisfaction', 'ifl_100_cabin_crew_satisfaction', 'pfl_500_boarding_satisfaction', 'ifl_300_cabin_satisfaction', 'bkg_200_journey_preparation_satisfaction']\n",
    "features=top_5_satisfaction_touchpoints + ['load_factor', 'mean_price', 'deviation_price'] + [col for col in merged_df.columns if col.startswith('otp')]\n",
    "                                   \n",
    "future_covariates_ts = TimeSeries.from_series(df[features])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ts, val_ts = nps_ts[:-12], nps_ts[-12:]\n",
    "train_covariates_ts, val_covariates_ts = future_covariates_ts[:-12], future_covariates_ts[-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the LightGBM model\n",
    "model = LightGBMModel(\n",
    "    lags=12,\n",
    "    quantiles=[0.25, 0.5, 0.75],\n",
    "    lags_future_covariates=[-12],  # Use the last 12 elements of covariate for each prediction step\n",
    "    likelihood=\"quantile\",\n",
    "    fit_intercept=False,\n",
    "    output_chunk_length=6\n",
    ")\n",
    "model.fit(train_ts, future_covariates=train_covariates_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions on the validation set\n",
    "pred_nps = model.predict(n=12, series=train_ts, future_covariates=train_covariates_ts)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "\n",
    "nps_ts.plot(label=\"Actual NPS\")\n",
    "pred_nps.plot(label=\"Forecast NPS\")\n",
    "plt.legend()\n",
    "plt.title(f\"PE_LH\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the plot as a PNG file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_5_satisfaction_touchpoints=['pun_100_punctuality_satisfaction', 'ifl_100_cabin_crew_satisfaction', 'pfl_500_boarding_satisfaction', 'ifl_300_cabin_satisfaction', 'bkg_200_journey_preparation_satisfaction']\n",
    "features=top_5_satisfaction_touchpoints + ['load_factor', 'mean_price', 'deviation_price'] + [col for col in merged_df.columns if col.startswith('otp')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futurecovariates_ts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Third approach: old school lagging + regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=grouped_dfs['Premium Economy_LH_df'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "# List of features to lag\n",
    "features_to_lag = (\n",
    "    ['NPS']\n",
    ")\n",
    "\n",
    "# Number of lags\n",
    "lags = 12\n",
    "\n",
    "# Create lagged features using pandas\n",
    "for feature in features_to_lag:\n",
    "    for lag in range(1, lags + 1):\n",
    "        df[f'lag_{lag}_{feature}'] = df[feature].shift(lag)\n",
    "\n",
    "# Fill NaN values with the median for each column\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Split into features and target variable\n",
    "X = df.drop(columns=['NPS','date_flight_local','cabin','haul'])\n",
    "y = df['NPS']\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create and train LightGBM model\n",
    "params = {\n",
    "    'objective': 'quantile',\n",
    "    'alpha': 0.25,  # Quantile parameter\n",
    "    'boosting_type': 'gbdt',\n",
    "    # Set other hyperparameters\n",
    "}\n",
    "\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Assuming you have already trained and evaluated the model\n",
    "# predictions contains the predicted NPS values on the validation set\n",
    "# y_val contains the actual NPS values on the validation set\n",
    "\n",
    "# Compute the MAE\n",
    "mae = mean_absolute_error(y_val, predictions)\n",
    "\n",
    "# Print the MAE\n",
    "print(f\"Mean Absolute Error: {mae}\")\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
